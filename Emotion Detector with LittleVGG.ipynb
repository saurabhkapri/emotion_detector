{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "import os\n",
    "\n",
    "num_classes=6\n",
    "img_rows,img_cols=48,48\n",
    "batch_size=16\n",
    "\n",
    "train_data_dir='C:\\\\Users\\\\kapri\\\\Desktop\\\\fer2013\\\\train'\n",
    "validation_data_dir='C:\\\\Users\\\\kapri\\\\Desktop\\\\fer2013\\\\validation'\n",
    "\n",
    "#doing some data argumentation\n",
    "train_datagen=ImageDataGenerator(\n",
    "                                rescale=1./255,\n",
    "                                rotation_range=30,\n",
    "                                shear_range=0.3,\n",
    "                                zoom_range=0.3,\n",
    "                                width_shift_range=0.3,\n",
    "                                height_shift_range=0.3,\n",
    "                                horizontal_flip=True)\n",
    "\n",
    "validation_datagen=ImageDataGenerator(\n",
    "                                     rescale=1./255)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 target_size=(img_rows,img_cols),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)\n",
    "validation_generator=validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                           color_mode='grayscale',\n",
    "                                                           target_size=(img_rows,img_cols),\n",
    "                                                           batch_size=batch_size,\n",
    "                                                           class_mode='categorical',\n",
    "                                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUR Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation,Flatten,Dropout,Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little VGG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_95 (Conv2D)           (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_100 (Bat (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_101 (Bat (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_102 (Bat (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_98 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_103 (Bat (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_99 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_104 (Bat (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_100 (Conv2D)          (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_105 (Bat (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_106 (Bat (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_107 (Bat (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, 3, 3, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_108 (Bat (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_104 (Conv2D)          (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_109 (Bat (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_110 (Bat (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_111 (Bat (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 12294     \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 7,368,678\n",
      "Trainable params: 7,358,566\n",
      "Non-trainable params: 10,112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "#1st block\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer=\"he_normal\",\n",
    "                 input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer=\"he_normal\",\n",
    "                 input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#2nd Block\n",
    "model.add(Conv2D(64,(3,3), padding='same', kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#3rd block\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#4th block\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# 5th block\n",
    "model.add(Conv2D(512,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(512,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6th block FC Layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2048,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1767/1767 [==============================] - 101s 57ms/step - loss: 2.6442 - accuracy: 0.1867 - val_loss: 2.0961 - val_accuracy: 0.2412\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.09615, saving model to emotion_LittleVGG.h5\n",
      "Epoch 2/10\n",
      "1767/1767 [==============================] - 103s 58ms/step - loss: 2.2833 - accuracy: 0.1975 - val_loss: 1.9035 - val_accuracy: 0.2501\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.09615 to 1.90350, saving model to emotion_LittleVGG.h5\n",
      "Epoch 3/10\n",
      "1767/1767 [==============================] - 106s 60ms/step - loss: 2.1017 - accuracy: 0.2021 - val_loss: 1.1024 - val_accuracy: 0.2339\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.90350 to 1.10235, saving model to emotion_LittleVGG.h5\n",
      "Epoch 4/10\n",
      "1767/1767 [==============================] - 97s 55ms/step - loss: 1.9726 - accuracy: 0.2070 - val_loss: 2.3864 - val_accuracy: 0.2476\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.10235\n",
      "Epoch 5/10\n",
      "1767/1767 [==============================] - 100s 57ms/step - loss: 1.9007 - accuracy: 0.2100 - val_loss: 1.7884 - val_accuracy: 0.2530\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.10235\n",
      "Epoch 6/10\n",
      "1767/1767 [==============================] - 105s 60ms/step - loss: 1.8416 - accuracy: 0.2198 - val_loss: 2.1346 - val_accuracy: 0.2462\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.10235\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 7/10\n",
      "1767/1767 [==============================] - 103s 59ms/step - loss: 1.7959 - accuracy: 0.2307 - val_loss: 1.8363 - val_accuracy: 0.2706\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.10235\n",
      "Epoch 8/10\n",
      "1767/1767 [==============================] - 110s 62ms/step - loss: 1.7862 - accuracy: 0.2374 - val_loss: 1.9539 - val_accuracy: 0.2723\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.10235\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD,RMSprop,Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "checkpoint=ModelCheckpoint('emotion_LittleVGG.h5',\n",
    "                          monitor='val_loss',\n",
    "                          mode='min',\n",
    "                          save_best_only=True,\n",
    "                          verbose=1)\n",
    "\n",
    "earlystop=EarlyStopping(monitor='val_loss',\n",
    "                       min_delta=0,\n",
    "                       patience=5,\n",
    "                       restore_best_weights=True,\n",
    "                        verbose=1)\n",
    "\n",
    "reduce_lr=ReduceLROnPlateau(monitor='val_loss',\n",
    "                           factor=0.2,\n",
    "                           patience=3,\n",
    "                           verbose=1,\n",
    "                           min_delta=0.0001)\n",
    "\n",
    "#putting callbacks in a list\n",
    "callbacks=[earlystop,checkpoint,reduce_lr]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(lr=0.0001),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "nb_train_samples=28273\n",
    "nb_validation_samples=3534\n",
    "epochs=10\n",
    "\n",
    "history=model.fit_generator(train_generator,\n",
    "                           steps_per_epoch=nb_train_samples//batch_size,\n",
    "                           epochs=epochs,\n",
    "                           callbacks=callbacks,\n",
    "                           validation_data=validation_generator,\n",
    "                           validation_steps=nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kapri\\anaconda3\\envs\\venv\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\kapri\\anaconda3\\envs\\venv\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\kapri\\anaconda3\\envs\\venv\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\kapri\\anaconda3\\envs\\venv\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "Confusion Matrix\n",
      "[[264  30  33  91  41  32]\n",
      " [ 99  85  43  93  78 130]\n",
      " [ 32  11 761  41   8  26]\n",
      " [109  33 172 160  66  86]\n",
      " [ 62  57  91 187 183  14]\n",
      " [ 18  22  29  12   2 333]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.45      0.54      0.49       491\n",
      "        Fear       0.36      0.16      0.22       528\n",
      "       Happy       0.67      0.87      0.76       879\n",
      "     Neutral       0.27      0.26      0.26       626\n",
      "         Sad       0.48      0.31      0.38       594\n",
      "    Surprise       0.54      0.80      0.64       416\n",
      "\n",
      "    accuracy                           0.51      3534\n",
      "   macro avg       0.46      0.49      0.46      3534\n",
      "weighted avg       0.48      0.51      0.48      3534\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier=load_model('emotion_LittleVGG.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting Class Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "{0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing on validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kapri\\anaconda3\\envs\\venv\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"\\\\\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = 'C:\\\\Users\\\\kapri\\\\Desktop\\\\fer2013\\\\validation\\\\' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing using webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(r'C:\\Users\\kapri\\Desktop\\age-gender-estimation\\Haarcascades\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4b62c608795e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rajeev.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mrects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-4b62c608795e>\u001b[0m in \u001b[0;36mface_detector\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Convert image to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfaces\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(r'C:\\Users\\kapri\\Desktop\\age-gender-estimation\\Haarcascades\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    allfaces = []   \n",
    "    rects = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "        allfaces.append(roi_gray)\n",
    "        rects.append((x,w,y,h))\n",
    "    return rects, allfaces, img\n",
    "\n",
    "img = cv2.imread(r\"\")\n",
    "rects, faces, image = face_detector(img)\n",
    "\n",
    "i = 0\n",
    "for face in faces:\n",
    "    roi = face.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "    # make a prediction on the ROI, then lookup the class\n",
    "    preds = classifier.predict(roi)[0]\n",
    "    label = class_labels[preds.argmax()]   \n",
    "\n",
    "    #Overlay our detected emotion on our pic\n",
    "    label_position = (rects[i][0] + int((rects[i][1]/2)), abs(rects[i][2] - 10))\n",
    "    i =+ 1\n",
    "    cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Emotion Detector\", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
